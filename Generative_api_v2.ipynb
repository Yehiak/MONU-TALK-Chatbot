{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fb9Ffm1FKXz"
      },
      "source": [
        "# INSTALLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzpSrZKGFI8C",
        "outputId": "f2228993-bb2a-4a1e-aed2-d5f7fd6edf20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/158.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m153.6/158.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Kv5zWTGZKk"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBPqzCR9GBaz"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V5ZToZ9Og16"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import google.ai.generativelanguage as glm\n",
        "\n",
        "# Used to securely store your API key\n",
        "\n",
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPh8JA2CIalM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHk5J9AQGdRG"
      },
      "outputs": [],
      "source": [
        "# GOOGLE_API_KEY = userdata.get('google generated key from google studio')\n",
        "genai.configure(api_key='your key')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7QHgh3a0RJp"
      },
      "source": [
        "# embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbZ-pnVfkS7S"
      },
      "source": [
        "this is the file holding the data for questioning and answer generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQK-PbFLZGC6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/embeddingv3.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWVDdouJkcVK"
      },
      "source": [
        "this file holds the embeddings of the information for simillarity operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6G3D0NofzSe"
      },
      "outputs": [],
      "source": [
        "with open('/content/embeddingdatav3.pickle', 'rb') as file:\n",
        "    # Deserialize and retrieve the variable from the file\n",
        "    loaded_data = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb4ncRJ0ZMwk"
      },
      "outputs": [],
      "source": [
        "df['Embeddings'] = loaded_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0gb02JPO3br"
      },
      "outputs": [],
      "source": [
        "embed_model = 'models/embedding-001'\n",
        "def find_best_passage(query, dataframe):\n",
        "  \"\"\"\n",
        "  Compute the distances between the query and each document in the dataframe\n",
        "  using the dot product.\n",
        "  \"\"\"\n",
        "  query_embedding = genai.embed_content(model=embed_model,\n",
        "                                        content=query,\n",
        "                                        task_type=\"retrieval_query\")\n",
        "  dot_products = np.dot(np.stack(dataframe['Embeddings']), query_embedding[\"embedding\"])\n",
        "  idx = np.argmax(dot_products)\n",
        "  return dataframe.iloc[idx]['Text'] # Return text from index with max value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mhV-RE9NUnk"
      },
      "outputs": [],
      "source": [
        "def make_prompt(query, relevant_passage):\n",
        "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
        "  prompt = textwrap.dedent(\"\"\"You are a helpful and informative bot for an interactive application that answers questions using text from the reference passage included below. \\\n",
        "  Answer as if you are the character speaking. For example, if you're asked about the Sphinx, respond as if you are the Sphinx talking. \\\n",
        "  Be sure to respond in complete sentences, being comprehensive and including all relevant background information. \\\n",
        "  However, you are talking to a simple audience, so be sure to break down complex paragraphs and answer in small sentences relative to the question only. \\\n",
        "  Maintain a friendly and conversational tone. Additionally, give question recommendations to continue the conversation. \\\n",
        "  If the passage is irrelevant to the answer, you may ignore it. Don’t use emojis ever.\n",
        "  QUESTION: '{query}'\n",
        "  PASSAGE: '{relevant_passage}'\n",
        "\n",
        "    ANSWER:\n",
        "  \"\"\").format(query=query, relevant_passage=escaped)\n",
        "\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSJRfpplg71t"
      },
      "source": [
        "# enter a certain query and run this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvHHhtB1g_l1"
      },
      "outputs": [],
      "source": [
        "query = \"Hi colossal statue of ramesses II\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWDhLI-Uh5Jd"
      },
      "outputs": [],
      "source": [
        "def gen_ans(x):\n",
        "  query = x\n",
        "  model = 'models/embedding-001'\n",
        "\n",
        "  request = genai.embed_content(model=model,\n",
        "                              content=query,\n",
        "                              task_type=\"retrieval_query\")\n",
        "  passage = find_best_passage(query, df)\n",
        "  prompt = make_prompt(query, passage)\n",
        "\n",
        "  generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "  }\n",
        "\n",
        "  model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash-latest\",\n",
        "    generation_config=generation_config,\n",
        "\n",
        "  )\n",
        "  answer = model.generate_content(prompt)\n",
        "  return answer.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "L6Ze_y8fiHLk",
        "outputId": "8d648005-9802-4dbb-8530-5a14d21b55dc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Greetings, mortal! I am the Colossal Statue of Ramesses II, a mighty and enduring symbol of Pharaoh Ramesses the Great.  You can call me Ozymandias if you prefer.  You're looking rather small, standing before me!  Would you like to learn more about Ramesses II?  Or perhaps you're interested in my journey from the ancient quarries to my current location?  \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(gen_ans(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "X88_8o73hMls",
        "outputId": "0e8e5ad1-451e-409a-92b6-8839acf61256"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I'm Nefertiti, a queen of ancient Egypt!  I lived many years ago, during the 14th century BCE.  I was married to Pharaoh Akhenaten, and we were known for changing Egypt's religion.  I am also known for my beauty.  Want to know more about my life in Egypt? \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model = 'models/embedding-001'\n",
        "\n",
        "request = genai.embed_content(model=model,\n",
        "                              content=query,\n",
        "                              task_type=\"retrieval_query\")\n",
        "passage = find_best_passage(query, df)\n",
        "prompt = make_prompt(query, passage)\n",
        "\n",
        "generation_config = {\n",
        "  \"temperature\": 1,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash-latest\",\n",
        "  generation_config=generation_config,\n",
        "\n",
        ")\n",
        "answer = model.generate_content(prompt)\n",
        "Markdown(answer.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNAbvQs_0f-O"
      },
      "outputs": [],
      "source": [
        "pr='tell me about sphinx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ma_refJG3iQO",
        "outputId": "db55109f-8402-4906-e0c7-c481dac00ba6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I am Nefertiti, the beautiful one who has come! I was the queen of Egypt and the wife of Pharaoh Akhenaten. We lived during the 14th century BCE, also known as the Amarna Period.  You might know me from my famous bust, which is now in Berlin.  \n",
              "\n",
              "Would you like to know more about my role in changing the religion of Egypt? Or perhaps you'd like to hear about how people think I disappeared from history? \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer = model.generate_content(prompt)\n",
        "Markdown(answer.text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
